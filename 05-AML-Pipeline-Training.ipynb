{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check core SDK version number\n",
    "import azureml.core\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "from azureml.core import (Datastore, Dataset, Environment, Experiment, ScriptRunConfig,\n",
    "                          Workspace)\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "\n",
    "print(\"[INFO] SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "## due to diferent tenant -> typically customer tenant\n",
    "# interactive_auth = InteractiveLoginAuthentication(tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\"[SUCCESS] LOGGED IN: \",ws.name, ws.resource_group, ws.location, ws.subscription_id, sep=' @ ')\n",
    "\n",
    "## set mlflow backend to AML\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "print(\"[INFO] MLFlow wired to AML:\", \"experiments.azureml.net\" in mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_compute = \"aml-cluster\"\n",
    "aml_ds = \"aml_data\"# \"mmaadlsgen2_test\"\n",
    "# aml_dset = 'noa_weather'\n",
    "# aml_dset = \"oj_sample_data\"\n",
    "aml_dset = \"diabetes_multiple\"\n",
    "aml_experiment = \"mlflow-azureml\"\n",
    "loc_data = \"data/demo_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the datastore\n",
    "ds = ws.datastores[aml_ds]\n",
    "print(f\"[INFO] Datastore: {ds.name}, type: {ds.datastore_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtds = Dataset.get_by_name(ws, name=aml_dset)\n",
    "# wtds = Dataset.get_by_name(ws, name='noa_weather')\n",
    "pdf = wtds.to_pandas_dataframe()\n",
    "pdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Comment this if the data visualisations doesn't work on your side\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = \"temperature\" # diabetes\n",
    "target = 'Y'\n",
    "\n",
    "categorical_features_list = ['SEX', target]\n",
    "quantitative_features_list = ['AGE', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [] # or empty\n",
    "cols_at_end = [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.drop(cols_to_drop,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just remove [] and the features with 30% or less NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(df[target].describe())\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.histplot(pdf[target], color='g', bins=100, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(pdf, test_size=0.2, random_state=223)\n",
    "\n",
    "print(f'train:{train.shape[0]} \\ntest: {test.shape[0]}')\n",
    "\n",
    "# final_df.to_csv(\"./data/taxi_final_df.csv\", index=False)\n",
    "# train.to_csv(\"./data/taxi_final_df_train.csv\", index=False)\n",
    "# test.to_csv(\"./data/taxi_final_df_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(aml_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, aml_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define RunConfig for the compute\n",
    "We will also use `pandas`, `scikit-learn` and `automl`, `pyarrow` for the pipeline steps. Defining the `runconfig` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a new runconfig object\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the aml_compute you created above. \n",
    "aml_run_config.target = aml_compute\n",
    "\n",
    "# Enable Docker\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['scikit-learn','packaging'], \n",
    "    pip_packages=['azureml-sdk', \n",
    "                    'pyarrow',\n",
    "                    'pandas==1.1.0',\n",
    "                    'azureml-dataprep[pandas,fuse]',\n",
    "                    'scikit-learn==0.22.2.post1',\n",
    "                    'azureml-mlflow',\n",
    "                    'lightgbm',\n",
    "                    'joblib'\n",
    "                 ])\n",
    "\n",
    "print (\"Run configuration created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "train_model_folder = \"code/train/diabetes\"\n",
    "\n",
    "print('Train script is in {}.'.format(os.path.realpath(train_model_folder)))\n",
    "\n",
    "# output_data = PipelineData(\"xxx\", datastore=default_store).as_dataset()\n",
    "\n",
    "\n",
    "# test train split step creation\n",
    "# See the train_test_split.py for details about input and output\n",
    "trainStep = PythonScriptStep(\n",
    "    name=\"Train Model\",\n",
    "    script_name=\"train.py\", \n",
    "    arguments=[\"--myarg\", 111],\n",
    "#     inputs=[output_split_train.parse_parquet_files(file_extension=None)],\n",
    "    inputs=[wtds.as_named_input('data')],\n",
    "#     outputs=[output_data],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig = aml_run_config,\n",
    "    source_directory=train_model_folder,\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "print(\"trainStep created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "pipeline_steps = [trainStep]\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)\n",
    "\n",
    "# pipeline.validate()\n",
    "\n",
    "print(\"Pipeline submitted for execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_run.get_all_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish  Pipeline\n",
    "\n",
    "When AML Pipeline is published, you can schedule such pipeline to run based on schedule trigger. Moreover, you can work with such Pipeline outside AML, e.g. you can shcedule the Pipeline in Azure Data Factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"Diabetes Training AML Pipeline\", description=\"training pipeline\", version=\"1.0\")\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PublishedPipeline\n",
    "published_pipelines = PublishedPipeline.list(ws)\n",
    "for published_pipeline in  published_pipelines:\n",
    "    print(f\"{published_pipeline.name},'{published_pipeline.id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the Pipeline in Azure Data Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such published Pipeline can be accessed from Azure Data Factory.\n",
    "\n",
    "1. Go to [https://ms-adf.azure.com/authoring](https://ms-adf.azure.com/authoring) and select your ADF\n",
    "1. If you don't have create Linked Service to your AML Workspace\n",
    "1. Create Pipeline with \"Machine Learning Execute Pipeline\" Activity:\n",
    "\n",
    "<img src=\"./media/adf-aml-1.png\" alt=\"Alt text that describes the graphic\" title=\"Title text\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

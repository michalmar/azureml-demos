{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check core SDK version number\n",
    "import azureml.core\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "from azureml.core import (Datastore, Dataset, Environment, Experiment, ScriptRunConfig,\n",
    "                          Workspace)\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "\n",
    "print(\"[INFO] SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "## due to diferent tenant -> typically customer tenant\n",
    "# interactive_auth = InteractiveLoginAuthentication(tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\"[SUCCESS] LOGGED IN: \",ws.name, ws.resource_group, ws.location, ws.subscription_id, sep=' @ ')\n",
    "\n",
    "## set mlflow backend to AML\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "print(\"[INFO] MLFlow wired to AML:\", \"experiments.azureml.net\" in mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_compute = \"aml-cluster\"\n",
    "aml_ds = \"aml_data\"# \"mmaadlsgen2_test\"\n",
    "# aml_dset = 'noa_weather'\n",
    "# aml_dset = \"oj_sample_data\"\n",
    "aml_dset = \"diabetes_multiple\"\n",
    "aml_experiment = \"mlflow-azureml\"\n",
    "loc_data = \"data/demo_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the datastore\n",
    "ds = ws.datastores[aml_ds]\n",
    "print(f\"[INFO] Datastore: {ds.name}, type: {ds.datastore_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtds = Dataset.get_by_name(ws, name=aml_dset)\n",
    "# wtds = Dataset.get_by_name(ws, name='noa_weather')\n",
    "pdf = wtds.to_pandas_dataframe()\n",
    "pdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Comment this if the data visualisations doesn't work on your side\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = \"temperature\" # diabetes\n",
    "target = 'Y'\n",
    "\n",
    "categorical_features_list = ['SEX', target]\n",
    "quantitative_features_list = ['AGE', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [] # or empty\n",
    "cols_at_end = [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.drop(cols_to_drop,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just remove [] and the features with 30% or less NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(df[target].describe())\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.histplot(pdf[target], color='g', bins=100, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(pdf, test_size=0.2, random_state=223)\n",
    "\n",
    "print(f'train:{train.shape[0]} \\ntest: {test.shape[0]}')\n",
    "\n",
    "# final_df.to_csv(\"./data/taxi_final_df.csv\", index=False)\n",
    "# train.to_csv(\"./data/taxi_final_df_train.csv\", index=False)\n",
    "# test.to_csv(\"./data/taxi_final_df_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(aml_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[quantitative_features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# df = pdf[quantitative_features_list]\n",
    "# y_df = df.pop(target)\n",
    "# x_df = df\n",
    "\n",
    "df = pdf\n",
    "x_df = df\n",
    "y_df = x_df.pop(target)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=223)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow Tracking\n",
    "\n",
    "```python\n",
    "mlflow.start_run()\n",
    "# mlflow.log_param(\"x\",\"abc\")\n",
    "mlflow.log_metric(\"x\",123)\n",
    "mlflow.end_run()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a run object in the experiment\n",
    "# run =  experiment.start_logging()\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Log the algorithm parameters to the run\n",
    "    # run.log('num_leaves', 31)\n",
    "    # run.log('learning_rate', 0.05)\n",
    "    # run.log('n_estimators', 20)\n",
    "    \n",
    "    num_leaves=31\n",
    "    learning_rate=0.01\n",
    "    n_estimators=20\n",
    "    \n",
    "    mlflow.log_param(\"num_leaves\",num_leaves)\n",
    "    mlflow.log_param(\"learning_rate\",learning_rate)\n",
    "    mlflow.log_param(\"n_estimators\",n_estimators)\n",
    "\n",
    "    # setup model, train and test\n",
    "    gbm = lgb.LGBMRegressor(num_leaves=num_leaves,\n",
    "                            learning_rate=learning_rate,\n",
    "                            n_estimators=n_estimators)\n",
    "    model_gbm = gbm.fit(x_train, y_train,\n",
    "            eval_set=[(x_test, y_test)],\n",
    "            eval_metric='l1',\n",
    "            early_stopping_rounds=5)\n",
    "\n",
    "    preds = model_gbm.predict(x_test)\n",
    "\n",
    "    # Output the Mean Squared Error to the notebook and to the run\n",
    "    print('Mean Squared Error is', mean_squared_error(y_test, preds))\n",
    "    # run.log('mse', mean_squared_error(y_test, preds))\n",
    "    mlflow.log_metric('mse', mean_squared_error(y_test, preds))\n",
    "\n",
    "#     # Save the model to the outputs directory for capture\n",
    "#     model_file_name = './outputs/model.pkl'\n",
    "\n",
    "#     joblib.dump(value = model_gbm, filename = model_file_name)\n",
    "\n",
    "    mlflow.sklearn.log_model(model_gbm, \"gbm_model\")\n",
    "\n",
    "    # upload the model file explicitly into artifacts \n",
    "    # run.upload_file(name = model_file_name, path_or_stream = model_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, aml_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List through individual `Run` metrics and sort output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# children = list(parent_run.get_children())\n",
    "metricslist = {}\n",
    "for run in experiment.get_runs():\n",
    "    if (run.status == \"Completed\"):\n",
    "    #     properties = run.get_properties()\n",
    "        metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}\n",
    "    #     metricslist[int(properties['iteration'])] = metrics\n",
    "    #     metricslist[run._run_number] = metrics\n",
    "        metricslist[run.id] = metrics\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).T.sort_values(by=['mse'], ascending=False)\n",
    "rundata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the **best** run by metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_rmse_runid = None\n",
    "minimum_rmse = None\n",
    "num_runs_compared = 0\n",
    "best_run = None\n",
    "\n",
    "for run in experiment.get_runs():\n",
    "    run_metrics = run.get_metrics()\n",
    "    run_details = run.get_details()\n",
    "    \n",
    "    if (run.status == \"Completed\"):\n",
    "        if (\"mse\" in run.get_metrics()):\n",
    "            num_runs_compared += 1\n",
    "            # each logged metric becomes a key in this returned dict\n",
    "            run_rmse = run_metrics[\"mse\"]\n",
    "            run_id = run_details[\"runId\"]\n",
    "\n",
    "            if minimum_rmse is None:\n",
    "                minimum_rmse = run_rmse\n",
    "                minimum_rmse_runid = run_id\n",
    "                best_run = run\n",
    "            else:\n",
    "                if run_rmse < minimum_rmse:\n",
    "                    minimum_rmse = run_rmse\n",
    "                    minimum_rmse_runid = run_id\n",
    "                    best_run = run\n",
    "\n",
    "print(\"Best run_id: \" + minimum_rmse_runid)\n",
    "print(\"Best run_id rmse: \" + str(minimum_rmse))\n",
    "print(\"Runs compared: \" + str(num_runs_compared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on AML Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "compute_target = ComputeTarget(workspace=ws, name=aml_compute)\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "cts = compute_target.status.serialize()\n",
    "print(f'Found existing compute target: {aml_compute}\\n({\"cluster is running\" if (int(cts[\"currentNodeCount\"])>0) else \"cluster is idle\"}) currentNodeCount: {cts[\"currentNodeCount\"]}, vmPriority: {cts[\"vmPriority\"]}, vmSize: {cts[\"vmSize\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_folder = \"code/train/diabetes\"\n",
    "\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)\n",
    "else:\n",
    "    print(f\"folder '{project_folder}' aready there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtds = Dataset.get_by_name(ws, name=aml_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "conda_env = Environment('conda-env')\n",
    "conda_env.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "                                                                             'azureml-dataprep[pandas,fuse]',\n",
    "                                                                             'scikit-learn==0.22.2.post1',\n",
    "                                                                             'azureml-mlflow',\n",
    "                                                                             'lightgbm',\n",
    "                                                                            'joblib'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder, \n",
    "                      script='train.py', \n",
    "                      arguments =[wtds.as_named_input('data')])\n",
    "\n",
    "src.run_config.framework = 'python'\n",
    "src.run_config.environment = conda_env\n",
    "src.run_config.target = compute_target.name\n",
    "# src.run_config.data_references = {ds.name: dr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(config=src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
